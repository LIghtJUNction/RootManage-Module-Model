import json
import os
import requests
from pathlib import Path
from datetime import datetime, timedelta
from typing import Any

class ProxyManagerMeta(type):
    """
    Metaclass for ProxyManager to ensure singleton behavior.
    """

    @property
    def ROOT(cls):
        """
        Returns the root directory of the proxy manager.
        """
        return Path(os.getenv("RMM_ROOT", Path.home() / "data" / "adb" / ".rmm"))

    @property
    def CACHE(cls):
        """
        Returns the cache dictionary for storing proxies.
        """
        CACHE = cls.ROOT / "CACHE"
        if not CACHE.exists():
            CACHE.mkdir(parents=True, exist_ok=True)
        return CACHE

    @property
    def PROXY_CACHE_FILE(cls):
        """
        Returns the path to the proxy cache file.
        """
        return cls.CACHE / "github_proxy.json"


class ProxyManager(metaclass=ProxyManagerMeta):
    """
    GitHubä»£ç†ç®¡ç†å™¨
    æ”¯æŒè‡ªåŠ¨è·å–å’Œç¼“å­˜GitHubä»£ç†åˆ—è¡¨
    """
    
    API_URL = "https://api.akams.cn/github"
    CACHE_DURATION = timedelta(hours=10)  # ç¼“å­˜10å°æ—¶
    
    @classmethod
    def _load_cache(cls) -> dict[str, Any] | None:
        """
        åŠ è½½ç¼“å­˜æ•°æ®
        
        Returns:
            dict[str, object] | None: ç¼“å­˜çš„ä»£ç†æ•°æ®ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ•ˆåˆ™è¿”å›None
        """
        try:
            if cls.PROXY_CACHE_FILE.exists():
                with open(cls.PROXY_CACHE_FILE, "r", encoding="utf-8") as f:
                    cache_data = json.load(f)
                    return cache_data
        except (json.JSONDecodeError, FileNotFoundError, KeyError) as e:
            print(f"âš ï¸  åŠ è½½ä»£ç†ç¼“å­˜å¤±è´¥: {e}")
        return None
    
    @classmethod
    def _save_cache(cls, data: dict[str, Any]) -> None:
        """
        ä¿å­˜æ•°æ®åˆ°ç¼“å­˜
        
        Args:
            data: è¦ç¼“å­˜çš„ä»£ç†æ•°æ®
        """
        try:
            with open(cls.PROXY_CACHE_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ä»£ç†ç¼“å­˜å·²ä¿å­˜åˆ°: {cls.PROXY_CACHE_FILE}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ä»£ç†ç¼“å­˜å¤±è´¥: {e}")
    
    @classmethod
    def _is_cache_valid(cls, cache_data: dict[str, object]) -> bool:
        """
        æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        
        Args:
            cache_data: ç¼“å­˜çš„æ•°æ®
            
        Returns:
            bool: ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            if not all(key in cache_data for key in ["cached_at", "data", "update_time"]):
                return False
            
            # æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ—¶é—´ï¼ˆ10å°æ—¶å†…ï¼‰
            cached_at = datetime.fromisoformat(cache_data["cached_at"])
            if datetime.now() - cached_at > cls.CACHE_DURATION:
                print("ğŸ•’ æœ¬åœ°ç¼“å­˜å·²è¶…è¿‡10å°æ—¶ï¼Œéœ€è¦æ›´æ–°")
                return False
            
            # æ£€æŸ¥APIæ›´æ–°æ—¶é—´ï¼ˆæ¯”è¾ƒæœåŠ¡å™¨æ›´æ–°æ—¶é—´ï¼‰
            api_update_time = cache_data.get("update_time", "")
            if api_update_time:
                try:
                    # è§£æAPIè¿”å›çš„æ›´æ–°æ—¶é—´
                    api_time = datetime.strptime(api_update_time, "%Y-%m-%d %H:%M:%S")
                    cached_time = datetime.strptime(cache_data.get("api_update_time", ""), "%Y-%m-%d %H:%M:%S")
                    
                    if api_time > cached_time:
                        print(f"ğŸ”„ æœåŠ¡å™¨æ•°æ®å·²æ›´æ–°ï¼ˆ{api_update_time}ï¼‰ï¼Œéœ€è¦åˆ·æ–°ç¼“å­˜")
                        return False
                except ValueError:
                    # æ—¶é—´è§£æå¤±è´¥ï¼Œè®¤ä¸ºç¼“å­˜æ— æ•ˆ
                    return False
            
            print(f"âœ… ç¼“å­˜æœ‰æ•ˆï¼Œæœ€åæ›´æ–°æ—¶é—´: {cache_data.get('update_time', 'Unknown')}")
            return True
            
        except Exception as e:
            print(f"âš ï¸  æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§æ—¶å‡ºé”™: {e}")
            return False
    
    @classmethod
    def _fetch_from_api(cls, timeout: int = 10) -> dict[str, object] | None:
        """
        ä»APIè·å–æœ€æ–°çš„ä»£ç†åˆ—è¡¨
        
        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            dict[str, object] | None: APIè¿”å›çš„æ•°æ®ï¼Œå¤±è´¥åˆ™è¿”å›None
        """
        try:
            print(f"ğŸŒ æ­£åœ¨ä»APIè·å–GitHubä»£ç†åˆ—è¡¨: {cls.API_URL}")
            
            response = requests.get(cls.API_URL, timeout=timeout)
            response.raise_for_status()
            
            api_data = response.json()
            
            # éªŒè¯APIå“åº”æ ¼å¼
            if api_data.get("code") != 200:
                print(f"âŒ APIè¿”å›é”™è¯¯: {api_data.get('msg', 'Unknown error')}")
                return None
            
            if "data" not in api_data:
                print("âŒ APIå“åº”ç¼ºå°‘dataå­—æ®µ")
                return None
            
            print(f"âœ… æˆåŠŸè·å– {api_data.get('total', 0)} ä¸ªä»£ç†")
            return api_data
            
        except requests.exceptions.Timeout:
            print(f"âŒ APIè¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡{timeout}ç§’ï¼‰")
        except requests.exceptions.RequestException as e:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
        except json.JSONDecodeError:
            print("âŒ APIå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
        except Exception as e:
            print(f"âŒ è·å–ä»£ç†åˆ—è¡¨æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        
        return None
    
    @classmethod
    def get_proxy_list(cls, force_update: bool = False, timeout: int = 10) -> list[dict[str, object]]:
        """
        è·å–GitHubä»£ç†åˆ—è¡¨
        
        Args:
            force_update: æ˜¯å¦å¼ºåˆ¶ä»APIæ›´æ–°ï¼Œå¿½ç•¥ç¼“å­˜
            timeout: APIè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            list[dict[str, object]]: ä»£ç†åˆ—è¡¨ï¼ŒåŒ…å«urlã€serverã€ipã€locationã€latencyã€speedç­‰å­—æ®µ
        """
        # 1. å°è¯•åŠ è½½ç¼“å­˜
        cache_data = None
        if not force_update:
            cache_data = cls._load_cache()
            
            # 2. æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§
            if cache_data and cls._is_cache_valid(cache_data):
                return cache_data["data"]
        
        # 3. ç¼“å­˜æ— æ•ˆæˆ–å¼ºåˆ¶æ›´æ–°ï¼Œä»APIè·å–
        api_data = cls._fetch_from_api(timeout)
        
        if api_data:
            # 4. ä¿å­˜æ–°çš„ç¼“å­˜
            cache_entry = {
                "cached_at": datetime.now().isoformat(),
                "api_update_time": api_data.get("update_time", ""),
                "update_time": api_data.get("update_time", ""),
                "total": api_data.get("total", 0),
                "data": api_data["data"]
            }
            cls._save_cache(cache_entry)
            return api_data["data"]
        
        # 5. APIå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨è¿‡æœŸç¼“å­˜
        if cache_data and "data" in cache_data:
            print("âš ï¸  APIè·å–å¤±è´¥ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜æ•°æ®")
            return cache_data["data"]
        
        # 6. å®Œå…¨å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
        print("âŒ æ— æ³•è·å–ä»£ç†åˆ—è¡¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []
    
    @classmethod
    def get_best_proxy(cls, force_update: bool = False) -> str | None:
        """
        è·å–æœ€ä½³çš„GitHubä»£ç†URLï¼ˆåŸºäºå»¶è¿Ÿå’Œé€Ÿåº¦ï¼‰
        
        Args:
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°ä»£ç†åˆ—è¡¨
            
        Returns:
            Optional[str]: æœ€ä½³ä»£ç†URLï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨ä»£ç†åˆ™è¿”å›None
        """
        proxy_list = cls.get_proxy_list(force_update)
        
        if not proxy_list:
            return None
          # æ ¹æ®å»¶è¿Ÿå’Œé€Ÿåº¦æ’åºï¼ˆå»¶è¿Ÿè¶Šä½è¶Šå¥½ï¼Œé€Ÿåº¦è¶Šé«˜è¶Šå¥½ï¼‰
        def score_proxy(proxy: dict[str, object]) -> float:
            latency = int(proxy.get("latency", 9999) or 9999)
            speed = float(proxy.get("speed", 0) or 0)
            # ç®€å•è¯„åˆ†ç®—æ³•ï¼šé€Ÿåº¦/å»¶è¿Ÿï¼Œå»¶è¿Ÿä¸º0æ—¶è®¾ä¸º1é¿å…é™¤é›¶
            return speed / max(latency, 1)
        
        best_proxy = max(proxy_list, key=score_proxy)
        print(f"ğŸš€ é€‰æ‹©æœ€ä½³ä»£ç†: {best_proxy['url']} (å»¶è¿Ÿ: {best_proxy.get('latency', 'N/A')}ms, é€Ÿåº¦: {best_proxy.get('speed', 'N/A')}MB/s)")
        
        return best_proxy["url"]
    
    @classmethod
    def clear_cache(cls) -> None:
        """
        æ¸…é™¤ä»£ç†ç¼“å­˜
        """
        try:
            if cls.PROXY_CACHE_FILE.exists():
                cls.PROXY_CACHE_FILE.unlink()
                print(f"âœ… å·²æ¸…é™¤ä»£ç†ç¼“å­˜: {cls.PROXY_CACHE_FILE}")
            else:
                print("â„¹ï¸  ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…é™¤")
        except Exception as e:
            print(f"âŒ æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
    
    @classmethod
    def get_cache_info(cls) -> dict[str, object]:
        """
        è·å–ç¼“å­˜ä¿¡æ¯
        
        Returns:
            dict[str, object]: ç¼“å­˜ä¿¡æ¯ï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€æ›´æ–°æ—¶é—´ç­‰
        """
        info = {
            "cache_file": str(cls.PROXY_CACHE_FILE),
            "exists": cls.PROXY_CACHE_FILE.exists(),
            "size": 0,
            "cached_at": None,
            "update_time": None,
            "total_proxies": 0
        }
        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨  
        if info["exists"]:
            try:
                info["size"] = cls.PROXY_CACHE_FILE.stat().st_size
                cache_data = cls._load_cache()
                if cache_data:
                    info["cached_at"] = cache_data.get("cached_at")
                    info["update_time"] = cache_data.get("update_time")
                    info["total_proxies"] = len(cache_data.get("data", []))
            except Exception as e:
                info["error"] = str(e)
        
        return info


# ä¾¿æ·å‡½æ•°  
def get_github_proxies(force_update: bool = False) -> list[dict[str, object]]:
    """
    è·å–GitHubä»£ç†åˆ—è¡¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
        
    Returns:
        list[dict[str, object]]: ä»£ç†åˆ—è¡¨
    """
    return ProxyManager.get_proxy_list(force_update)


def get_best_github_proxy(force_update: bool = False) -> str | None:
    """
    è·å–æœ€ä½³GitHubä»£ç†çš„ä¾¿æ·å‡½æ•°
    
    Args:
        force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
        
    Returns:
        Optional[str]: æœ€ä½³ä»£ç†URL
    """
    return ProxyManager.get_best_proxy(force_update)


# ç¤ºä¾‹ç”¨æ³•å’Œæµ‹è¯•
if __name__ == "__main__":
    # æµ‹è¯•ä»£ç†ç®¡ç†å™¨
    print("ğŸ§ª æµ‹è¯•GitHubä»£ç†ç®¡ç†å™¨")
    print("=" * 50)
    
    # è·å–ç¼“å­˜ä¿¡æ¯
    print("ğŸ“‹ ç¼“å­˜ä¿¡æ¯:")
    cache_info = ProxyManager.get_cache_info()
    for key, value in cache_info.items():
        print(f"   {key}: {value}")
    
    print("\n" + "=" * 50)
    
    # è·å–ä»£ç†åˆ—è¡¨
    print("ğŸ“¦ è·å–ä»£ç†åˆ—è¡¨:")
    proxies = get_github_proxies()
    
    if proxies:
        print(f"âœ… æ‰¾åˆ° {len(proxies)} ä¸ªä»£ç†:")
        for i, proxy in enumerate(proxies, 1):
            print(f"   {i}. {proxy.get('url', 'N/A')} - "
                  f"å»¶è¿Ÿ: {proxy.get('latency', 'N/A')}ms, "
                  f"é€Ÿåº¦: {proxy.get('speed', 'N/A')}MB/s")
        
        # è·å–æœ€ä½³ä»£ç†
        print(f"\nğŸ¯ æœ€ä½³ä»£ç†: {get_best_github_proxy()}")
    else:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨ä»£ç†")
