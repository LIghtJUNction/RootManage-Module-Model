"""GitHubä»£ç†ç®¡ç†å™¨ - è·å–å’Œè§£æGitHubä»£ç†èŠ‚ç‚¹"""
import json
import requests
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any
from urllib.parse import urlparse

@dataclass
class ProxyNode:
    """ä»£ç†èŠ‚ç‚¹æ•°æ®ç±»"""
    url: str
    speed: float
    
    def get_domain_main_part(self) -> str:
        """è·å–åŸŸåçš„ä¸»ä½“éƒ¨åˆ†ï¼Œå»é™¤å‰ç¼€å¦‚github."""
        parsed = urlparse(self.url)
        domain = parsed.netloc
        # ç§»é™¤å¸¸è§çš„githubç›¸å…³å‰ç¼€
        if domain.startswith('github.'):
            return domain[7:]  # ç§»é™¤ 'github.'
        elif domain.startswith('gh.'):
            return domain[3:]   # ç§»é™¤ 'gh.'
        elif domain.startswith('ghproxy.'):
            return domain[8:]   # ç§»é™¤ 'ghproxy.'
        elif domain.startswith('ghp.'):
            return domain[4:]   # ç§»é™¤ 'ghp.'
        elif 'github' in domain and '.' in domain:
            # å¯¹äºåŒ…å«githubçš„åŸŸåï¼Œå–æœ€åçš„ä¸»åŸŸåéƒ¨åˆ†
            parts = domain.split('.')
            return '.'.join(parts[-2:]) if len(parts) >= 2 else domain
        return domain

class ProxyManager:
    """GitHubä»£ç†ç®¡ç†å™¨"""    
    API_URL = "https://api.akams.cn/github"
    @classmethod
    def get_proxies(cls) -> list[ProxyNode]:
        """
        è·å–GitHubä»£ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ŒæŒ‰é€Ÿåº¦æ’åº
        
        Returns:
            list[ProxyNode]: æŒ‰é€Ÿåº¦é™åºæ’åˆ—çš„ä»£ç†èŠ‚ç‚¹åˆ—è¡¨
            
        Raises:
            requests.RequestException: ç½‘ç»œè¯·æ±‚å¤±è´¥
            json.JSONDecodeError: JSONè§£æå¤±è´¥
            ValueError: æ•°æ®æ ¼å¼é”™è¯¯
        """
        # å‘é€HTTPè¯·æ±‚
        response: requests.Response = requests.get(cls.API_URL, timeout=30)
        response.raise_for_status()        # è§£æJSONå“åº”
        response_data: dict[str, Any] = response.json()

        data_list = response_data['data']

        # è§£æä»£ç†èŠ‚ç‚¹
        proxies: list[ProxyNode] = []        
        for item_data in data_list:  # type: ignore

            url = item_data["url"]
            speed = item_data["speed"]


            proxies.append(ProxyNode(url=url, speed=speed))
        # æŒ‰é€Ÿåº¦é™åºæ’åº
        proxies.sort(key=lambda x: x.speed, reverse=True)
        
        return proxies
    
    @classmethod
    def save_proxies_to_file(cls, proxies: list[ProxyNode], file_path: Path) -> None:
        """
        å°†ä»£ç†åˆ—è¡¨ä¿å­˜åˆ°æ–‡ä»¶
        
        Args:
            proxies: ä»£ç†èŠ‚ç‚¹åˆ—è¡¨
            file_path: ä¿å­˜æ–‡ä»¶çš„è·¯å¾„
            
        Raises:
            OSError: æ–‡ä»¶å†™å…¥å¤±è´¥
            json.JSONEncodeError: JSONç¼–ç å¤±è´¥
        """
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å°†ä»£ç†æ•°æ®è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        proxy_data = [asdict(proxy) for proxy in proxies]
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(proxy_data, f, ensure_ascii=False, indent=2)
    
    @classmethod
    def load_proxies_from_file(cls, file_path: Path) -> list[ProxyNode]:
        """
        ä»æ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨
        
        Args:
            file_path: ä»£ç†æ–‡ä»¶è·¯å¾„
            
        Returns:
            list[ProxyNode]: ä»£ç†èŠ‚ç‚¹åˆ—è¡¨
            
        Raises:
            FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
            json.JSONDecodeError: JSONè§£æå¤±è´¥
            ValueError: æ•°æ®æ ¼å¼é”™è¯¯
        """
        if not file_path.exists():
            raise FileNotFoundError(f"ä»£ç†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            proxy_data: list[dict[str,str|float]] = json.load(f)
        # è½¬æ¢ä¸ºProxyNodeå¯¹è±¡
        proxies: list[ProxyNode] = []
        for item in proxy_data:  # type: ignore
            
            url: str = item['url'] if isinstance(item['url'], str) else str(item['url'])

            speed:float = item['speed'] if isinstance(item['speed'], (int, float)) else float(item['speed'])

            proxies.append(ProxyNode(url=url, speed=speed))
        return proxies

    @classmethod
    def get_and_save_proxies(cls, project_path: Path) -> tuple[list[ProxyNode], Path]:
        """
        è·å–ä»£ç†åˆ—è¡¨å¹¶ä¿å­˜åˆ°é¡¹ç›®çš„ .rmmp/rmmp.proxys æ–‡ä»¶
        
        Args:
            project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
            
        Returns:
            tuple[list[ProxyNode], Path]: (ä»£ç†åˆ—è¡¨, ä»£ç†æ–‡ä»¶è·¯å¾„)
            
        Raises:
            requests.RequestException: ç½‘ç»œè¯·æ±‚å¤±è´¥
            OSError: æ–‡ä»¶æ“ä½œå¤±è´¥
        """
        # è·å–ä»£ç†åˆ—è¡¨
        proxies = cls.get_proxies()
        
        # ç¡®å®šä¿å­˜è·¯å¾„
        proxy_file = project_path / ".rmmp" / "rmmp.proxys"
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        cls.save_proxies_to_file(proxies, proxy_file)
        
        return proxies, proxy_file
    
    @classmethod
    def load_project_proxies(cls, project_path: Path) -> list[ProxyNode]:
        """
        ä»é¡¹ç›®çš„ä»£ç†æ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨
        
        Args:
            project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
            
        Returns:
            list[ProxyNode]: ä»£ç†èŠ‚ç‚¹åˆ—è¡¨ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        proxy_file = project_path / ".rmmp" / "rmmp.proxys"
        try:
            return cls.load_proxies_from_file(proxy_file)
        except FileNotFoundError:
            return []
    
    @classmethod
    def generate_proxy_download_links(cls, project_path: Path, download_url: str, max_proxies: int = 10) -> str:
        """
        ç”Ÿæˆä»£ç†ä¸‹è½½é“¾æ¥çš„markdownæ–‡æœ¬ï¼Œç”¨äºå‘å¸ƒè¯´æ˜
        
        Args:
            project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
            download_url: åŸå§‹ä¸‹è½½é“¾æ¥
            max_proxies: æœ€å¤šæ˜¾ç¤ºçš„ä»£ç†æ•°é‡ï¼Œé»˜è®¤10ä¸ª
            
        Returns:
            str: åŒ…å«ä»£ç†ä¸‹è½½é“¾æ¥çš„markdownæ–‡æœ¬
        """
        proxies = cls.load_project_proxies(project_path)
        if not proxies:
            return ""
        
        # é™åˆ¶ä»£ç†æ•°é‡
        top_proxies = proxies[:max_proxies]
          # ç”Ÿæˆmarkdownæ–‡æœ¬
        proxy_links: list[str] = []
        proxy_links.append("## ğŸš€ åŠ é€Ÿä¸‹è½½é“¾æ¥")
        proxy_links.append("")
        proxy_links.append("ä»£ç†ä¸‹è½½åœ°å€åˆ—è¡¨ï¼Œå·²æŒ‰é€Ÿåº¦æ’åºï¼š")
        proxy_links.append("")
        
        for i, proxy in enumerate(top_proxies, 1):
            domain_main = proxy.get_domain_main_part()
            speed_text = f"{proxy.speed:.1f}Mb/s" if proxy.speed > 0 else "æµ‹é€Ÿä¸­"
            proxy_download_url = download_url.replace("github.com", proxy.url.replace("https://", "").replace("http://", ""))
            proxy_links.append(f"{i}. [{domain_main} ({speed_text})]({proxy_download_url})")
        
        proxy_links.append("")
        proxy_links.append("*ä»£ç†èŠ‚ç‚¹ç”±ç¬¬ä¸‰æ–¹æä¾›ï¼Œé€Ÿåº¦ä»…ä¾›å‚è€ƒ*")
        
        return "\n".join(proxy_links)